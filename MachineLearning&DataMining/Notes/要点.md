# 知识要点



## 1 绪论

### 1.1 监督学习与无监督学习的区别

|              | 监督学习                     | 无监督学习                         |
| ------------ | ---------------------------- | ---------------------------------- |
| **数据**     | 带标签数据                   | 无标签数据                         |
| **反馈**     | 直接反馈                     | 无反馈                             |
| **目标**     | 预测结果/未来                | 查找数据中的隐藏结构               |
| **主要任务** | 分类和回归                   | 聚类、降维、异常检测等             |
| **优点**     | 预测性能好，能处理复杂任务   | 无需标注数据，适用于探索性数据分析 |
| **缺点**     | 需要大量数据，数据标注成本高 | 结果的解释性较弱，难以验证模型效果 |



### 1.2 机器学习的具体流程

1. **收集数据集，标记数据**

2. **拆分数据集**

   - 训练集：训练模型，确定模型/网络参数
   - 验证/开发集：调整超参数（学习率，正则化参数 等），选择特征，以及对学习算法作出其它决定
   - 测试集：评估算法的性能

   通常将训练集/验证集/测试集的比例设为60%:20%:20%，但们没一般规则指明多少比例合适，依赖于训练样本的容量和数据信噪比。

   训练集 ∩ 验证/开发集 ∩ 测试集 = ∅

3. **训练模型**：对于深度学习，随机梯度降

4. **评价**：查准率(精度)，查全率(召回率) ，F1-度量



### 1.3 k折交叉验证法

k 折交叉验证法是一种常用的<u>模型评估技术</u>，主要用于评估机器学习模型的性能和稳定性。

1. **数据集划分**：将数据集分为k个大小相同的子集。

2. **模型训练和测试**：每次使用其中一个子集作为测试集，其余作为训练集。

3. **计算性能指标**：在每次训练和测试后，计算模型在测试集上的<u>性能指标</u>。

4. **重复**这个过程k次，每次选择不同的子集作为测试集。

5. **结果汇总**：将k次的性能指标进行<u>平均</u>，以获得模型的整体性能评估结果。

k折交叉验证法的优点：稳定性高，充分利用数据



### 1.4 过拟合产生的原因

1. 模型复杂度过高，拥有太多参数相对于训练数据量。
2. 训练数据量不足
3. 训练数据包含噪声，模型学习到了这些噪声特征。
4. 训练时间过长，模型过度学习训练数据的特性





## 2 数据与特征工程

### 2.1 什么是数据？

- **数据集**：一组含有特征（属性）的数据对象（样本）的集合
- **样本**：由一组特征（属性）所描述的一个对象
- **特征（属性）**：描述样本或对象在某方面的表现或性质的事项



### 2.2 相似度与相异度

相似度：

- 两个样本之间<u>相似程度</u>的**数值化**度量
- 两个样本越相似，他们之间的相似性就越高
- 相似度是**非负的**，通常取值范围在[0, 1]

相异度：

- 两个样本之间<u>差异程度</u>的**数值化**度量
- 两个样本越相似，他们之间的相异性越低
- 相异度是**非负的**，取值在[0,1]和[0, ∞)均有

> 相似性计算见[<u>计算题</u>](计算题.md/#1相似性计算)。



### 2.3 特征的类型

#### 定性属性：不具有数的大部分性质

- **标称类型**：只能区分样本之间的不同。 *例如：学号、籍贯、邮政编*  
- **序数类型**：能够对样本之间的顺序进行区分。 *例如：排名、年级、衣服的号码 {S, M, L, XL, XXL}*

#### 定量属性：用数表示，并且具有数的大部分性质，可以是连续值或整数值

- **区间类型**：能够对样本在坐标系上的相对距离进行度量。*例如：日历上的日期、摄氏或华氏温度等*

- **比率类型**：能够对样本在坐标系上的绝对位置进行标定。*例如：开尔文温度、 长度、时间、质量、货币单位等*



### 2.4 特征工程、特征提取、特征选择之间的区别

- **特征工程**：从已有数据中**创建新的特征**；**注入领域知识**。
- **特征提取**：将原始数据**转换**为特征，会**创建新特征**；**降维**过程。
- **特征选择**：选择特征**子集**；**不创建新特征**。





## 3 线性模型

### 3.1 线性模型

$$
f(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + b
$$

其中，$\mathbf{x} = [x_1, x_2, \ldots, x_d]^T$ 是由属性性描述的样本，$x_i$ 是 $\mathbf{x}$ 在第 $i$ 个属性上的取值（即属性值） 。$\mathbf{w} = (w_1; w_2; \ldots; w_d) = [w_1, w_2, \ldots, w_d]^T$。



### 3.2 线性回归模型的优点

- 形式简单、易于建模
- 可解释性 (权重系数$\mathbf{w}$直观地表达了各属性在预测中的重要性)
- 非线性模型的基础



### 3.3 模型/参数估计：最小二乘法

**代价函数 (cost function)**  $L(y, f(x))$ 来<u>度量误差</u>。回归任务中，**代价(损失)函数(cost/loss function)** 普遍使用 **<u>均方误差</u>**。基于均方误差最小化进行模型求解的方法称为 **“最小二乘法”** 。

均方误差：
$$
E(w, b) = \sum_{i=1}^{m} (y_i - wx_i - b)^2
$$

最小化均方误差：
$$
\arg \min_{(w,b)}E(w,b) = \arg \min_{(w,b)} \sum_{i=1}^{m} (y_i - wx_i - b)^2
$$
分别对 $w$ 和 $b$ 求偏导，可得
$$
\frac{\partial E(w, b)}{\partial w} = 2 \left( w \sum_{i=1}^{m} x_i^2 - \sum_{i=1}^{m} (y_i - b) x_i \right)
$$

$$
\frac{\partial E(w, b)}{\partial b} = 2 \left( mb - \sum_{i=1}^{m} (y_i - wx_i) \right)
$$

得到封闭形式的解
$$
w = \dfrac{\sum_{i=1}^{m} y_i (x_i - \overline{x})}{\sum_{i=1}^{m} x_i^2 - \frac{1}{m} \left( \sum_{i=1}^{m} x_i \right)^2}
$$

$$
b = \dfrac{1}{m} \sum_{i=1}^{m} (y_i - wx_i)
$$

其中均值

$$
\overline{x} = \frac{1}{m} \sum_{i=1}^{m} x_i
$$



### 3.4 对数回归模型

**对数几率 (log odds)**：样本作为正例的相对可能性的对数。
$$
\ln \left( \frac{y}{1-y} \right) = w^Tx + b
$$


把 <u>对数几率函数(sigmoid函数)</u> 作为联系函数 $g^{-1}(\cdot)$：
$$
y = \frac{1}{1+e^{-z}} \quad \text{其中} \quad z = w^Tx + b
$$

$$
y = \frac{1}{1+e^{-(w^Tx+b)}}
$$



### 3.5 对数几率回归的优点

- 无需事先假设数据分布
- 可得到“类别”的近似概率预测
- 可直接应用现有数值优化算法求取最优解



### 3.6 最大似然法

对率回归模型最大化“对数似然” 等价于 **最小化负对数似然**：

令 $\beta = (w; b), \hat{x} = (x; 1)$，则 $w^T x + b$ 可简写为 $\beta^T \hat{x}$

$$
\begin{align*}
\ell(\beta) &= -\ln L(\beta) \\
&= -\sum_{i=1}^{m} \left[ y_i \ln h_{\beta}(x_i) + (1 - y_i) \ln (1 - h_{\beta}(x_i)) \right] \\
&= \sum_{i=1}^{m} \left[ -y_i \beta^T \hat{x}_i + \ln \left( 1 + e^{\beta^T \hat{x}_i} \right) \right] \quad
\end{align*}
$$
其中，$h_{\beta}(x_i)$ 为 $p(y=1 \mid x)$，$1 - h_\beta(x)$ 为 $p(y=0 \mid x)$。



### 3.7 对率回归模型求解

#### 牛顿法

牛顿法参数 $\beta$ 更新公式：
$$
\beta^{t+1} = \beta^t - \left( \frac{\partial^2 \ell(\beta)}{\partial \beta \partial \beta^T} \right)^{-1} \frac{\partial \ell(\beta)}{\partial \beta}
$$

#### 梯度下降法

<u>梯度</u>: 损失函数的等高线的**法线**方向

梯度下降法参数 $\beta$ 的更新公式：
$$
\beta^{t+1} = \beta^t - \alpha \frac{\partial \ell(\beta)}{\partial \beta}
$$
其中 $\alpha$ 是学习率。

> 其中，
>
> 其中关于 $\beta$ 的一阶、二阶导数分别为
> $$
> \frac{\partial \ell(\beta)}{\partial \beta} = -\sum_{i=1}^{m} \hat{x}_i (y_i - p_1 (\hat{x}_i; \beta)) \quad
> $$
>
> $$
> \frac{\partial^2 \ell(\beta)}{\partial \beta \partial \beta^T} = \sum_{i=1}^{m} \hat{x}_i \hat{x}_i^T p_1 (\hat{x}_i; \beta) (1 - p_1 (\hat{x}_i; \beta)) \quad
> $$



### 3.8 LDA的基本思想

给定训练样例集，设法将样例投影到一条**适当选择**的直线上，使得同类样例的投影点尽可能接近、异类样例的投影点中心尽可能远离。

> *“投影后类内方差小，类间距离大”*

> LDA计算见[<u>计算题</u>](计算题.md)。



### 3.9 LDA和PCA的区别

相同点：

- 两者均可以对数降维
- 两者在降维时均使用了矩阵特征分解的思想
- 两者都假设数据符合高斯分布

不同点：

- LDA是有监督的，PCA是无监督的
- LDA降维最多降到k-1维，PCA无限制
- LDA除了降维还可以分类
- LDA选择**分类性能最好**的投影方向，PCA选择样本点**投影具有最大方差**的方向





## 4 决策树

### 4.1 信息熵

**信息熵**定义如下：
$$
Ent(D) = -\sum_{k=1}^{|\mathcal{Y}|} p_k \log_2 p_k \\
D : [x_1, x_2, \ldots, x_m] \qquad P : [p_1, p_2, \ldots, p_m]
$$

$Ent(D)$ 的值越小，则 $D$ 的纯度越高，样本的确定性越高。熵越大，随机变量的不确定性就越大。



### 4.2 划分选择：3种决策树的最优化分属性度量

#### 4.2.1 算法ID3 - 信息增益

- **信息增益**：

$$
Gain(D, a) = Ent(D) - \sum_{v=1}^{V} \frac{|D^v|}{|D|} Ent(D^v) = Ent(D) - Ent(D|a)
$$

- **存在的问题**：信息增益对<u>可取数目较多的属性</u>有所偏好

- **ID3** 核心思想：选择**信息增益最大的属性** $a_p$ 作为**最优划分属性**（设为根结点），根据 $a_p$ 将数据集分成 $v_p$ 个子集 $\{D^{a_p^1}, D^{a_p^2}, \cdots, D^{a_p^{v_p}}\}$;

#### 4.2.2 算法C4.5 - 增益率

- **增益率**：

$$
Gain\_ratio(D, a) = \frac{Gain(D, a)}{IV(a)}
$$

- **固有值**：
  $$
  IV(a) = - \sum_{v=1}^{V} \frac{|D^v|}{|D|} \log_2 \frac{|D^v|}{|D|}
  $$

  属性 $a$ 的可能取值数目越多（即 $V$ 越大），则 $IV(a)$ 的值通常就越大。

- **存在的问题**：增益率准则对<u>可取值数目较少的属性</u>有所偏好

- **C4.5** 使用了一个启发式： 先从候选划分属性中找出**信息增益**高于<u>平均水平</u>的属性，再从中选取**增益率**最高的。

#### 4.2.3 CART决策树 - 基尼指数

- **基尼值**：
  $$
  Gini(D) = \sum_{k=1}^{|\mathcal{Y}|} \sum_{k' \neq k}^{|\mathcal{Y}|} p_k p_{k'} = \sum_{k=1}^{|\mathcal{Y}|} p_k (1 - p_k) = 1 - \sum_{k=1}^{|\mathcal{Y}|} p_k^2
  $$

  基尼值越小，数据纯度越高。

- **基尼指数**：
  $$
  Gini\_index(D, a) = \sum_{v=1}^{V} \frac{|D^v|}{|D|} Gini(D^v)
  $$
  **基尼指数（基尼不纯度）**表示在样本集合中一个随机选中的样本<u>被分错的概率</u>。

- **存在的问题**：Gini指数的划分趋向于**孤立数据集中数量多的类**，将它们分到一个树叶中。
- **CART决策树** 核心思想：选择那个使划分后**基尼指数最小的属性**作为**最优划分属性**。

#### 4.2.4 ID3、C4.5和CART决策树的主要区别

|                    | ID3                    | C4.5                     | CART                         |
| ------------------ | ---------------------- | ------------------------ | ---------------------------- |
| **划分属性**       | 信息增益               | 信息增益率               | 基尼指数                     |
| **处理数据类型**   | 离散型数据             | 连续型和离散型数据       | 连续型和离散型数据           |
| **是否构建平衡树** | 是                     | 是                       | 否                           |
| **生成树类型**     | 多叉树                 | 多叉树                   | 二叉树                       |
| **剪枝方法**       | 不包含剪枝             | 包含剪枝                 | 包含剪枝                     |
| **缺失值处理**     | 无法处理缺失值         | 可处理缺失值             | 可处理缺失值                 |
| **存在的问题**     | 偏好可取数目较多的属性 | 偏好可取值数目较少的属性 | 趋向于孤立数据集中数量多的类 |

> - 基于熵的算法偏向于构建一颗平衡树 (ID3、C4.5)
> - 基于基尼值的算法偏向构建不平衡的树 (CART)



### 4.3 为什么剪枝？

**剪枝** 是决策树学习算法对付 **过拟合** 的主要手段。

可通过 **剪枝** 来一定程度避免因决策 **分支过多**，以致于把训练集自身的<u>一些特点</u>当做所有数据都具有的<u>一般性质</u>而导致的 **过拟合**。



### 4.4 判断决策树泛化性能是否提升的方法

**留出法**：预留一部分数据用作“验证集”以进行性能评估



### 4.5 剪枝的基本策略

#### 4.5.1 预剪枝

决策树生成过程中，对每个结点在划分前先进行估计，若<u>当前结点的划分</u>**不能**带来决策树泛化性能提升，则**停止划分**并将当前结点记为叶结点，其<u>类别标记为训练样例数最多的类别</u>。

#### 4.5.2 后剪枝

先从训练集生成一棵完整的决策树，然后**自底向上**地对非叶结点进行考察，若将该结点对应的<u>子树替换为叶结点</u>**能**带来决策树泛化性能提升，则将该**子树替换为叶结点**。



### 4.6 预剪枝和后剪枝的优缺点

#### 预剪枝的优缺点

- 优点
  - 预剪枝让决策树的很多分支没有展开，**降低了过拟合风险**
  - 显著**减少训练时间和测试时间开销**
- 缺点
  - **欠拟合风险**：有些分支的当前划分虽然不能提升泛化性能，但在其基础上进行的后续划分却有可能导致性能显著提高。预剪枝基于“贪心”本质禁止这些分支展开，带来了欠拟合风险。

#### 后剪枝的优缺点

- 优点
  - 后剪枝比预剪枝保留了更多的分支，**欠拟合风险小**，**泛化性能**往往优于预剪枝决策树
- 缺点
  - **训练时间开销大**：后剪枝过程是在生成完全决策树之后进行的，需要自底向上对所有非叶结点逐一考察



### 4.7 连续值处理的方法

连续值离散化——**二分法**。

- 将连续属性的取值排序后找到所有相邻值的中点作为候选的划分点。
- 采用离散属性值方法，对于这些划分点选择最优化分点（信息增益最大的点）。

与离散属性不同，若当前结点划分属性为连续属性，**该属性还可作为其后代结点的划分属性**。只是两次划分点的取值可能不同。



### 4.8 缺失值处理的方法

#### 如何在属性值缺失的情况下进行<u>划分属性选择</u>？

> 划分对象——属性

计算划分损失减少值时，**忽略特征缺失的样本**，最终计算的值乘以**比例**（实际参与计算的样本数除以总的样本数）

#### 给定划分属性，若样本在该属性上的值缺失，如何<u>对样本进行划分</u>？

> 划分对象——样本

将该样本**分配到所有子结点中**，权重由1变为具有属性*a*的样本被划分成的子集样本个数的相对比率，计算错误率的时候，需要考虑到样本权重。（相当于让同一个样本<u>以不同概率划入不同的子结点中</u>去)

#### 训练完成，给测试集样本分类，有缺失值怎么办？

> 测试集样本划分

- 如果决策没有用到缺失变量，则决策过程和没有缺失的数据一样
- 如果决策要用到缺失变量，决策树在当前结点做多数投票来决定（**选择样本数最多的特征值方向**）





## 5 神经网络

### 5.1 为什么使用非线性激活函数？

- 如果**激活函数是线性的**，则神经网络的工作原理与**线性变换**相同，这意味着深度神经网络(DNN) 退化成一个**线性模型**，没有足够的能力去建模许多复杂类型的数据。
- 用**非线性**激活函数，多层神经网络可以逼近更复杂的函数。



### 5.2 克服神经网络分类器过拟合的方法

1. 使用正则化技术，如L1、L2正则化
2. 应用dropout技术，随机丢弃一些神经元
3. 数据增强，通过变化训练样本来增加数据多样性



### 5.3 前向传播 vs 反向传播

在前馈神经网络中

- 前向传播
  - 从输入 $\mathbf{x}$ 到输出 $\mathbf{y}$，信息通过网络<u>前向传播</u>
  - 在训练阶段, 前向传播可以持续向前传递，直到产生代价函数 $C(θ)$
- 反向传播
  - 允许来自代价函数的信息通过网络<u>反向流动</u>, 以便计算**梯度**
  - 可以被应用到任何函数



### 5.4 反向传播算法

1. **输入样本 $x$**：为输入层设置对应的激活值 $a^1$。

2. **前向传播**：对每个 $l = 2, 3, ..., L$ 计算相应的 $z^l = w^l a^{l-1} + b^l$ 和 $a^l = \sigma(z^l)$

3. **输出层误差 $\delta^L$**：计算向量 $\delta^L = \nabla_a C \odot \sigma'(z^L)$

4. **反向误差传播**：对每个 $l = L-1, L-2, ..., 2$，计算 $\delta^l = ((\mathbf{w}^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l)$

5. **输出**：代价函数的梯度由 $\dfrac{\partial C}{\partial w_{jk}^l} = a_k^{l-1} \delta_j^l$ 和 $\dfrac{\partial C}{\partial b_j^l} = \delta_j^l$ 得出  



### 5.5 梯度降优化求解（批量梯度降 vs 随机梯度降 vs 小批量梯度降）

- 批量（batch）梯度降：每一次迭代都需要**所有样本的梯度**参与计算，求和平均后迭代更新权重和偏置
- 随机梯度降（SGD）：每一次迭代只使用**一个样本的梯度**参与计算，再迭代更新权重和偏置
- 小批量（Mini-batch）梯度降：每一次迭代选出**m个样本的梯度**参与计算，求和平均后迭代更新权重和偏置（m = batch size）



### 5.6 CNN中的通道：多通道图像卷积操作的通道一致性

- 每一个<u>卷积核(滤波)的通道数</u>必须和<u>输入图像的通道数</u> **相同**。
- 每次卷积操作后的<u>输出的通道数</u>和<u>滤波的个数</u> **相同**。



### 5.7 为什么使用CNN处理图像？

1. **图像中的一些模式比整个图像小得多**：网络中神经元的作用是检测图像中有无某些重要的模式，不需要把整个图像作为输入即可检测图像中某些关键模式。
   - 不同的神经元可具有**相同的感受野**
   - 不同的神经元的感受野可以**重叠**
2. **相同的模式出现在图像的不同区域**：可使用相同的检测器（滤波），参数共享
   - 参数共享：不同感受野的两个不同神经元的权重和偏置共享。
3. **下采样图像不改变**：下采样图像以使图像更小，但不影响人对图像目标的理解。
   - 处理图像的<u>网络参数</u>较少。



### 5.8 CNN中各层的作用

- 卷积层：卷积层通过局部连接核权值共享的方法提取一些初级视觉特征
- 池化层：将视觉特征筛选并结合成更高级、抽象的视觉特征
- 全连接层：在CNN中起“分类器”的作用，将学习到的分布式特征表示映射到样本标记空间
- 激活函数：添加非线性变化



### 5.9 池化层在CNN中的作用

- **保留主要特征**的同时**减少参数**和**计算量**

- 在一定程度上能**防止过拟合**：由于这一层没有参数，不需要学习。

- 提高**特征的不变性**：这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)。这就使网络的鲁棒性增强了，有一定抗扰动的作用。




### 5.10 1 × 1卷积的作用

- 在不改变空间维度的情况下，**增加或减少通道数**。
- 提高网络的**非线性表达能力**。
- 实现**跨通道的信息整合**。





## 6 SVM

### 6.1 SVM的思想

- 使得边界点距离分类超平面最大，这个方法就是**支持向量机 (SVM)**
- SVM 就是寻找一个最优的决策边界，距离两个类别最近的样本最远，其中最近的样本点称为**支持向量**。
- SVM算法就是 **最大化间隔**。



### 6.2 原始SVM的最优化问题

1. 设已知训练集 $\mathbf{D} = \{(x_1, y_1), (x_2, y_2), \ldots, (x_m, y_m)\}$，其中 $x_i \in \mathbb{R}^d$，$y_i \in \mathcal{Y} = \{1, -1\}$，$i = 1, 2, \ldots, m$

2. 构造并求解凸二次规划

   $$
   \min_{\mathbf{w}, b} \|\mathbf{w}\|^2_2 \quad \text{s.t.} \quad y_i (\mathbf{w}^T \mathbf{x}_i + b) \ge 1, \quad i = 1, 2, \ldots, m
   $$
   求得解 $\mathbf{w}^*, b^*$

3. 构造划分超平面 $(\mathbf{w}^*)^T \mathbf{x} + b^* = 0$，由此得到决策函数

$$
f(x) = \text{sgn}((\mathbf{w}^*)^T \mathbf{x} + b^*)
$$

$$
\text{sgn}(a) = 
  \begin{cases} 
  1, & \text{if } a \ge 0, \\
  -1, & \text{if } a < 0.
  \end{cases}
$$

> 计算题见 [<u>计算题</u>](计算题.md)。



### 6.3 为什么要从原始问题转换成对偶问题求解？

1. **约束转换**：对偶问题将原始问题中的不等式约束转为了等式约束。
2. **复杂度变化**：对偶问题的复杂度依赖于样本数量 $m$ 而非样本维度 $d$，对于高维数据集，计算效率更高。此外，最终只需要计算支持向量，大大减少计算量。
3. **稀疏性**：对偶问题的解具有稀疏性，只涉及支持向量的计算，使得求解更加高效。
4. **核函数引入**：对偶问题便于引入核函数，从而推广到非线性分类问题。



### 6.4 KKT条件

$$
\begin{cases}
\alpha_i \ge 0, \\
y_i f(x_i) - 1 \ge 0, \\
\alpha_i (y_i f(x_i) - 1) = 0.
\end{cases}
$$

满足这三个条件的解就是**局部极小值点**。

 $\alpha$ 系数<u>只在**支持向量才非零**</u>，<u>其它全部为 0</u>。



### 6.5 SMO的变量选择策略

- **选择违背 KKT 条件的那些变量先更新。**

  由于最终所有计算得到的 $a_i$ 都会满足 KKT 条件, 因此如果存在某个 $a_i$ 不满足 KKT 条件, 那么目标函数会最快衰减。

- **使选取的两变量所对应样本之间的间隔隔最大。**

  比较各变量所对应的目标函数值减幅的复杂度过高, 因此 SMO 采用这个启发式。这样的两个变量有很大的差别, 与对两个相似的变量进行更新相比, 对它们进行更新会带给目标函数值更大的变化



### 6.6 线性可分支撑向量分类的算法流程

1. 设已知训练集 $\mathbf{D} = \{(x_1, y_1), (x_2, y_2), \ldots, (x_m, y_m)\}$，其中 $x_i \in \mathbb{R}^d$，$y_i \in \mathcal{Y} = \{1, -1\}$，$i = 1, 2, \ldots, m$

2. 构造并利用SMO算法求解凸二次规划
   $$
   \min_{\alpha} \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j - \sum_{i=1}^m \alpha_i \quad \text{s.t.} \quad \sum_{i=1}^m \alpha_i y_i = 0, \alpha_i \ge 0, i = 1, 2, \ldots, m
   $$
   求得解 $\alpha^* = [\alpha_1^*, \alpha_2^*, \ldots, \alpha_m^*]^T$

3. 使用所有支持向量计算得到的 $b$ 的均值记为最优值，即
   $$
   b^* = \frac{1}{|S|} \sum_{s \in S} \left( \frac{1}{y_s} - \sum_{i \in S} \alpha_i y_i \mathbf{x}_i^T \mathbf{x}_s \right), \quad S = \{i | \alpha_i > 0, i = 1, 2, \ldots, m\}
   $$

4. 构造划分超平面 $(\mathbf{w}^*)^T \mathbf{x} + b^* = 0$，由此得到决策函数
   $$
   f(x) = \text{sgn}(g(x)) \quad \text{sgn}(a) = 
   \begin{cases} 
   1, & \text{if } a \ge 0, \\
   -1, & \text{if } a < 0.
   \end{cases}
   $$
   其中 $g(x) = (\mathbf{w}^*)^T \mathbf{x} + b^* = \sum_{i=1}^m \alpha_i^* y_i \mathbf{x}_i^T \mathbf{x} + b^*$



### 6.7 核函数的作用

1. **提高计算效率**：引入核函数，把高维向量的内积转变成了求低维向量的内积问题。
2. **降低计算复杂度**：核函数是一种<u>表征映射</u>、<u>实现内积逻辑关系</u>且<u>降低计算复杂度</u>的一类特殊函数。
3. **映射到高维空间**：通过隐式映射到高维空间，使得原本线性不可分的数据变得线性可分。
4. **灵活性**：增加模型的灵活性，能够适应各种类型的数据分布。





## 7 关联分析
