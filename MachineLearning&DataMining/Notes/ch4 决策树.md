# ch4 决策树



# A. 基本概念



## I. 决策树基本概念

### 1 定义

**决策树**（decision tree）模型常常用来解决**<u>分类</u>**和**<u>回归</u>**问题。

**定义**：**决策树**是对数据样本进行分类的一种树型数据结构。

### 2 决策树的组成部分

**决策树包含**：

- **结点**

  - **根结点**：没有入边，但有零条或多条出边

  - **内部结点**：恰有一条入边和两条或多条出边

  - **叶子结点/终结点**：恰有一条入边，但没有出边

- **有向边**

每个叶子结点都赋予一个类标记（**决策结果**）。

从树的根结点到叶子结点的一条**路径**，就代表一条**决策规则**。

### 3 决策树的目的

**决策树学习的目的**：为了产生一颗泛化能力强的决策树，即处理未见样本能力强。

**总结**：决策树中包含的 if...then... 规则应该与数据不矛盾，并且具有较强的**泛化性能**。



## II. 信息及信息熵

决策树的构造依赖于信息论。

- **信息**：用来消除某种随机不确定性的东西。
- **信息熵**：衡量随机变量的不确定性的某种度量（复杂度）。

**信息熵**定义如下：
$$
Ent(D) = -\sum_{k=1}^{|\mathcal{Y}|} p_k \log_2 p_k \\
D : [x_1, x_2, \ldots, x_m] \qquad P : [p_1, p_2, \ldots, p_m]
$$

这里 $D$ 代表当前样本（状态）集合，$P$ 代表概率集合，假定当前样本集合 $D$ 中第 $k$ 类样本 $x_k$ 所在比例为 $p_k$ ($k = 1, 2, \ldots, |\mathcal{Y}|$)。$|\mathcal{Y}|$ 表示类别个数。

- 信息熵是度量样本集合纯度最常用的一种指标
- $Ent(D)$ 的值越小，则 $D$ 的纯度越高，样本的确定性越高。熵越大，随机变量的不确定性就越大。





# B. 划分选择



## I. 信息增益

离散属性 $a$ 有 $V$ 个可能的取值 $\{a^1, a^2, ..., a^V\}$，用 $a$ 来进行划分，则会产生 $V$ 个分支结点，其中第 $v$ 个分支结点包含了 $D$ 中所有在属性 $a$ 上取值为 $a^v$ 的样本，记为 $D^v$。则可计算出用属性 $a$ 对样本集 $D$ 进行划分所获得的**“信息增益”**：

$$
Gain(D, a) = Ent(D) - \sum_{v=1}^{V} \frac{|D^v|}{|D|} Ent(D^v) = Ent(D) - Ent(D|a)
$$

$$
Ent(D) = -\sum_{k=1}^{|\mathcal{Y}|} p_k \log_2 p_k
$$

一般而言，**信息增益越大**，则意味着使用属性 $a$ 来进行划分所获得的**“纯度提升”**越大（信息增益代表了在一个条件下，信息复杂度（不确定性）减少的程度）

**迭代二分器（ID3）**决策树学习算法以**信息增益**为准则来选择**划分属性**。



### ID3 算法

输入：训练集 $D = \{(x_1, y_1), (x_2, y_2), \cdots, (x_m, y_m)\}$

属性集 $A = \{a_1, a_2, \cdots, a_d\}$

输出：决策树 $T : TreeGenerate(D, A)$

**STEP 1**: 若 $D$ 中所有样本都属于同一类 $C$，则 $T$ 为单结点树，并将 $C$ 作为该结点的类标记，返回 $T$，否则转到 **STEP 2**；

**STEP 2**: 依据决策属性计算信息熵 $Ent(D)$，令 $k = 1$，

1. 选择 $a_k$，假设 $a_k$ 具有 $v_k$ 个可能的取值，$D^{a_k^i}$ 为属性 $a_k = v_k^i$ 的样本集合，计算**条件信息熵** $Ent(D|a_k) = \sum_{i=1}^{v_k} \frac{|D^{a_k^i}|}{|D|} Ent(D^{a_k^i})$

2. 计算 $a_k$ 属性的**信息增益**，$Gain(D, a_k) = Ent(D) - Ent(D|a_k)$；

3. $k = k + 1$，若 $k < m$，则跳转到 1；

**STEP 3**: ==选择信息增益最大的属性 $a_p$ 设为根结点==，根据 $a_p$ 将数据集分成 $v_p$ 个子集 $\{D^{a_p^1}, D^{a_p^2}, \cdots, D^{a_p^{v_p}}\}$;

**STEP 4**: 令 $D = D^{a_p^j}, A = A - a_p$，转 **STEP 1**。



**存在的问题**：信息增益对可取数目较多的属性有所偏好



## II. 增益率

**增益率**定义：
$$
Gain\_ratio(D, a) = \frac{Gain(D, a)}{IV(a)}
$$

其中

$$
IV(a) = - \sum_{v=1}^{V} \frac{|D^v|}{|D|} \log_2 \frac{|D^v|}{|D|}
$$

称为属性 $a$ 的 "固有值"，属性 $a$ 的可能取值数目越多（即 $V$ 越大），则 $IV(a)$ 的值通常就越大。



**存在的问题**：增益率准则对可取值数目较少的属性有所偏好  



### C4.5 算法

**C4.5** 使用了一个启发式： 

先从候选划分属性中找出**信息增益**高于<u>平均水平</u>的属性，再从中选取**增益率**最高的。



## III. 基尼指数

> 基尼指数计算可以避免ID3, C4.5中的大量对数运算，计算更快。

数据集 $D$ 的纯度可用 **“基尼值”** 来度量
$$
Gini(D) = \sum_{k=1}^{|\mathcal{Y}|} \sum_{k' \neq k}^{|\mathcal{Y}|} p_k p_{k'} = \sum_{k=1}^{|\mathcal{Y}|} p_k (1 - p_k) = 1 - \sum_{k=1}^{|\mathcal{Y}|} p_k^2
$$

$p_k (k=1,2,\ldots,|\mathcal{Y}|)$ 表示选中的样本属于类别 $k$ 的概率，那么这个样本被分错的概率是 $1 - p_k$。

反映了从 $D$ 中随机抽取两个样本，其类别标记不一致的概率。

$Gini(D)$ 越小，数据集越集中，$D$ 的纯度越高。

属性 $a$ 的**基尼指数**定义为：
$$
Gini\_index(D, a) = \sum_{v=1}^{V} \frac{|D^v|}{|D|} Gini(D^v)
$$

**基尼指数（基尼不纯度）**：表示在样本集合中一个随机选中的样本被分错的概率

应选择那个使划分后**基尼指数最小的属性**作为**最优划分属性**，即

$$
a_* = \arg \min_{a \in A} Gini\_index(D, a)
$$
Gini指数的划分趋向于**孤立数据集中数量多的类**，将它们分到一个树叶中。而熵偏向于构建一颗**平衡的树**，也就是数量多的类可能**分散**到不同的叶子中去了。



### CART 决策树

CART分类树算法使用**基尼系数**来代替信息增益率，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益(率)是相反的。

CART作为分类树时，**特征属性**可以是**连续类型**也可以是**离散类型**，但**标签属性**(或者分类属性)必须是**离散类型**。





# C. 剪枝处理



## I. 为什么剪枝

**“剪枝”** 是决策树学习算法对付 **“过拟合”** 的主要手段

可通过“剪枝”来一定程度避免因决策**分支过多**，以致于把训练集自身的<u>一些特点</u>当做所有数据都具有的<u>一般性质</u>而导致的**过拟合**

判断决策树泛化性能是否提升的方法：

- **留出法**：预留一部分数据用作“验证集”以进行性能评估



## II. 剪枝的基本策略

### 1 预剪枝

决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点记为叶结点，其类别标记为训练样例数最多的类别。

分别计算划分前（即直接将该结点作为叶结点）及划分后的**验证集精度**，判断是否需要划分。

- 若划分后能**提高验证集精度**，则划分，对划分后的每一个属性，执行同样判断；
- 否则，不划分

若最终得到仅有一层划分的决策树，称为 “**决策树桩**“。

#### 预剪枝的优缺点

- 优点
  - 预剪枝让决策树的很多分支没有展开, **降低了过拟合风险**
  - 显著**减少训练时间和测试时间开销**
- 缺点
  - **欠拟合风险**：有些分支的当前划分虽然不能提升泛化性能，但在其基础上进行的后续划分却有可能导致性能显著提高。预剪枝基于“贪心”本质禁止这些分支展开，带来了欠拟合风险。

### 2 后剪枝

先从训练集生成一棵完整的决策树，然后**自底向上**地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点.

#### 后剪枝的优缺点

- 优点
  - 后剪枝比预剪枝保留了更多的分支，**欠拟合风险小**，**泛化性能**往往优于预剪枝决策树
- 缺点
  - **训练时间开销大**：后剪枝过程是在生成完全决策树之后进行的，需要自底向上对所有非叶结点逐一考察





# D. 连续与缺失值



## I. 连续值处理

利用连续属性离散化技术-**二分法**，对连续属性进行处理。

连续属性离散化 - 二分法：

- **第一步**：假设属性 $a$ 在样本集 $D$ 上出现 $n$ 个不同的取值，从小到大排列，记为 $a^1, a^2, \ldots, a^n$，基于划分点 $t$，可将 $D$ 分为子集 $D^-_t$ 和 $D^+_t$，其中 $D^-_t$ 包含那些在属性 $a$ 上取值不大于 $t$ 的样本，$D^+_t$ 包含那些在属性 $a$ 上取值大于 $t$ 的样本。构造包含 $n-1$ 个元素的候选划分点集合：
  $$
  T_a = \left\{ \frac{a^i + a^{i+1}}{2} \mid 1 \leq i \leq n-1 \right\}
  $$

  即把区间 $[a^i, a^{i+1})$ 的中位点 $\frac{a^i + a^{i+1}}{2}$ 作为候选划分点 $t$。

- **第二步**：采用离散属性值方法，考察这些划分点，选取最优的划分点进行样本集合的划分：

  $$
  \begin{aligned}
  \text{Gain}(D, a) &= \max_{t \in T_a} \text{Gain}(D, a, t) \\
  &= \max_{t \in T_a} \left( \text{Ent}(D) - \sum_{\lambda \in \{-, +\}}   \frac{|D^\lambda_t|}{|D|} \text{Ent}(D^\lambda_t) \right) \\
  &= \max_{t \in T_a} \left( \text{Ent}(D) - \left( \frac{|D^-_t|}{|D|}   \text{Ent}(D^-_t) + \frac{|D^+_t|}{|D|} \text{Ent}(D^+_t) \right)       \right)
  \end{aligned}
  $$

  其中 $\text{Gain}(D, a, t)$ 是样本集 $D$ 基于划分点 $t$ 二分后的信息增益，于是，就可选择使 $\text{Gain}(D, a, t)$ 最大化的划分点。

与离散属性不同，**若当前结点划分属性为连续属性**，**该属性还可作为其后代结点的划分属性**。只是两次划分点的取值可能不同。



## II. 缺失值处理

### 如何在属性值缺失的情况下进行划分属性选择？

➢ **计算划分损失减少值时，忽略特征缺失的样本，最终计算的值乘以比例（实际参与计算的样本数除以总的样本数）**

$\tilde{D}$ 表示 $D$ 中在属性 $a$ 上没有缺失值的样本子集, $\tilde{D}^v$ 表示 $D$ 中在属性 $a$ 上取值为 $a^v$ 的样本子集, $\tilde{D}_k$ 表示 $\tilde{D}$ 中属于第 $k$ 类的样本子集

假定为每个样本 $x$ 赋予一个权重 $w_x$，在训练刚开始时，根结点中各样本的权重 $w_x$ 初始化为 $1$ 

并定义：

- 无缺失值样本所占的比例
$$
\rho = \frac{\sum_{x \in \tilde{D}} w_x}{\sum_{x \in D} w_x}
$$

- 无缺失值样本中第 $k$ 类所占比例
$$
\tilde{p}_k = \frac{\sum_{x \in \tilde{D}_k} w_x}{\sum_{x \in \tilde{D}} w_x} \quad (1 \leq k \leq |\mathcal{Y}|)
$$

- 无缺失值样本中在属性 $a$ 上取值为 $a^v$ 的样本所占比例
$$
\tilde{r}_v = \frac{\sum_{x \in \tilde{D}^v} w_x}{\sum_{x \in \tilde{D}} w_x} \quad (1 \leq v \leq V)
$$

- 基于上述定义可得推广后的信息增益计算式：
$$
\text{Gain}(D, a) = \rho \times \text{Gain}(\tilde{D}, a)
= \rho \times \left( \text{Ent}(\tilde{D}) - \sum_{v=1}^{V} \tilde{r}_v \text{Ent}(\tilde{D}^v) \right)
$$

其中
$$
\text{Ent}(\tilde{D}) = -\sum_{k=1}^{|\mathcal{Y}|} \tilde{p}_k \log_2 \tilde{p}_k
$$

### 给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？

➢ **将该样本分配到所有子结点中，权重由1变为具有属性*a*的样本被划分成的子集样本个数的相对比率，计算错误率的时候，需要考虑到样本权重**

- 若样本 $x$ 在划分属性 $a$ 上的取值已知，则将 $x$ 划入与其取值对应的子结点，且样本权值在子结点中保持为 $w_x$
- 若样本 $x$ 在划分属性 $a$ 上的取值未知，则将 $x$ 同时划入所有子结点，且样本权值在与属性值 $a^v$ 对应的子结点中调整为 $\tilde{r}_v \cdot w_x$ (直观来看，相当于让同一个样本<u>以不同概率划入不同的子结点中</u>去)

### 训练完成，给测试集样本分类，有缺失值怎么办？

分类时，如果待分类样本有缺失变量，而决策树决策过程中没有用到这些变量，则决策过程和没有缺失的数据一样；否则，如果决策要用到缺失变量，决策树也可以在当前结点做多数投票来决定（**选择样本数最多的特征值方向**）
