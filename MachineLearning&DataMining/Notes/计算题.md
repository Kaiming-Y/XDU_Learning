# 计算题



## 1 相似性计算

### 1) 简单系数匹配

处理<u>对称</u>的二值离散型属性
$$
\text{SMC} = \frac{\text{值匹配的属性个数}}{\text{属性个数}} 
= \frac{f_{11} + f_{00}}{f_{01} + f_{10} + f_{11} + f_{00}}
$$

### 2) Jaccard 系数

处理包含<u>非对称</u>的二值离散型属性
$$
J = \frac{\text{匹配个数}}{\text{00匹配中不涉及的属性个数}} 
= \frac{f_{11}}{f_{01} + f_{10} + f_{11}}
$$

例题：

<img src="https://raw.githubusercontent.com/abecedarian007/picgo_images/master/img/20240527221026.png" style="zoom: 33%;" />

### 3) 余弦相似度

$$
\cos(\mathbf{x}, \mathbf{y}) = \frac{\langle \mathbf{x}, \mathbf{y} \rangle}{\|\mathbf{x}\| \|\mathbf{y}\|}
$$

其中，
$$
\langle \mathbf{x}, \mathbf{y} \rangle = \sum_{k=1}^{n} x_k y_k = \mathbf{x}^T \mathbf{y}
$$

$$
\|\mathbf{x}\| = \sqrt{ \sum_{k=1}^{n} x_k^2 } = \sqrt{ \langle \mathbf{x}, \mathbf{x} \rangle } = \sqrt{ \mathbf{x}' \mathbf{x} } \\
\|\mathbf{y}\| = \sqrt{ \sum_{k=1}^{n} y_k^2 } = \sqrt{ \langle \mathbf{y}, \mathbf{y} \rangle } = \sqrt{ \mathbf{y}' \mathbf{y} }
$$

例题：

<img src="https://raw.githubusercontent.com/abecedarian007/picgo_images/master/img/20240527221443.png" style="zoom: 33%;" />

### 4) 皮尔森相关系数

度量两个变量之间的**线性相关性**：<u>协方差与标准差之比</u>
$$
\text{corr}(\mathbf{x}, \mathbf{y}) = \frac{\text{covariance}(\mathbf{x}, \mathbf{y})}{\text{standard\_deviation}(\mathbf{x}) \times \text{standard\_deviation}(\mathbf{y})} = \frac{s_{xy}}{s_x s_y}
$$

$$
\text{covariance}(\mathbf{x}, \mathbf{y}) = s_{xy} = \frac{1}{n-1} \sum_{k=1}^{n} (x_k - \overline{x})(y_k - \overline{y})
$$

$$
\text{standard\_deviation}(\mathbf{x}) = s_x = \sqrt{ \frac{1}{n-1} \sum_{k=1}^{n} (x_k - \overline{x})^2 }
$$

$$
\text{standard\_deviation}(\mathbf{y}) = s_y = \sqrt{ \frac{1}{n-1} \sum_{k=1}^{n} (y_k - \overline{y})^2 }
$$

$$
\overline{x} = \frac{1}{n} \sum_{k=1}^{n} x_k \quad \text{是} \quad \mathbf{x} \quad \text{的均值}
$$

$$
\overline{y} = \frac{1}{n} \sum_{k=1}^{n} y_k \quad \text{是} \quad \mathbf{y} \quad \text{的均值}
$$

皮尔森系数的局限性：对于**非线性的相关性**难以建模

例题：

<img src="https://raw.githubusercontent.com/abecedarian007/picgo_images/master/img/20240527223255.png" style="zoom: 33%;" />





## 2 LDA 计算

#### 题目

由两类二维数据计算线性判别分析(LDA)投影向量

第一类采样数据 $ \omega_1 $ ： $\mathbf{X_1} = (\mathbf{x_1}, \mathbf{x_2}) = \{(4,2), (2,4), (2,3), (3,6), (4,4)\} \quad$

第二类采样数据 $ \omega_2 $ ： $\mathbf{X_2} = (\mathbf{x_1}, \mathbf{x_2}) = \{(9,10), (6,8), (9,5), (8,7), (10,8)\} \quad$

<img src="https://raw.githubusercontent.com/abecedarian007/picgo_images/master/img/20240529203718.png" style="zoom:50%;" />

#### 解

1. 计算**均值**
   $$
   \mu_1 = \frac{1}{N_1} \sum_{\mathbf{x} \in \omega_1} \mathbf{x} = \frac{1}{5} \left[ \left( \begin{array}{c} 4 \\ 2 \end{array} \right) + \left( \begin{array}{c} 2 \\ 4 \end{array} \right) + \left( \begin{array}{c} 2 \\ 3 \end{array} \right) + \left( \begin{array}{c} 3 \\ 6 \end{array} \right) + \left( \begin{array}{c} 4 \\ 4 \end{array} \right) \right] = \left( \begin{array}{c} 3 \\ 3.8 \end{array} \right)
   $$

   $$
   \mu_2 = \frac{1}{N_2} \sum_{\mathbf{x} \in \omega_2} \mathbf{x} = \frac{1}{5} \left[ \left( \begin{array}{c} 9 \\ 10 \end{array} \right) + \left( \begin{array}{c} 6 \\ 8 \end{array} \right) + \left( \begin{array}{c} 9 \\ 5 \end{array} \right) + \left( \begin{array}{c} 8 \\ 7 \end{array} \right) + \left( \begin{array}{c} 10 \\ 8 \end{array} \right) \right] = \left( \begin{array}{c} 8.4 \\ 7.6 \end{array} \right)
   $$

2. 计算**类内散度矩阵**
   $$
   \begin{aligned}
   S_1 &= \sum_{\mathbf{x} \in \omega_1} (\mathbf{x} - \mu_1)(\mathbf{x} - \mu_1)^T \\
   &= \left[ \left( \begin{array}{c} 4 \\ 2 \end{array} \right) - \left( \begin{array}{c} 3 \\ 3.8 \end{array} \right) \right]^2 + \left[ \left( \begin{array}{c} 2 \\ 4 \end{array} \right) - \left( \begin{array}{c} 3 \\ 3.8 \end{array} \right) \right]^2 + \left[ \left( \begin{array}{c} 2 \\ 3 \end{array} \right) - \left( \begin{array}{c} 3 \\ 3.8 \end{array} \right) \right]^2 + \left[ \left( \begin{array}{c} 3 \\ 6 \end{array} \right) - \left( \begin{array}{c} 3 \\ 3.8 \end{array} \right) \right]^2 + \left[ \left( \begin{array}{c} 4 \\ 4 \end{array} \right) - \left( \begin{array}{c} 3 \\ 3.8 \end{array} \right) \right]^2 \\
   &= \left( \begin{array}{cc} 4 & -1 \\ -1 & 8.8 \end{array} \right)
   \end{aligned}
   $$

   $$
   \begin{aligned}
   S_2 &= \sum_{\mathbf{x} \in \omega_2} (\mathbf{x} - \mu_2)(\mathbf{x} - \mu_2)^T \\
   &= \left[ \left( \begin{array}{c} 9 \\ 10 \end{array} \right) - \left( \begin{array}{c} 8.4 \\ 7.6 \end{array} \right) \right]^2 + \left[ \left( \begin{array}{c} 6 \\ 8 \end{array} \right) - \left( \begin{array}{c} 8.4 \\ 7.6 \end{array} \right) \right]^2 + \left[ \left( \begin{array}{c} 9 \\ 5 \end{array} \right) - \left( \begin{array}{c} 8.4 \\ 7.6 \end{array} \right) \right]^2 + \left[ \left( \begin{array}{c} 8 \\ 7 \end{array} \right) - \left( \begin{array}{c} 8.4 \\ 7.6 \end{array} \right) \right]^2 + \left[ \left( \begin{array}{c} 10 \\ 8 \end{array} \right) - \left( \begin{array}{c} 8.4 \\ 7.6 \end{array} \right) \right]^2 \\
   &= \left( \begin{array}{cc} 9.2 & -0.2 \\ -0.2 & 13.2 \end{array} \right)
   \end{aligned}
   $$

3. 计算**总类内散度矩阵**
   $$
   \begin{aligned}
   S_w &= S_1 + S_2 \\
   &= \left( \begin{array}{cc} 4 & -1 \\ -1 & 8.8 \end{array} \right) + \left( \begin{array}{cc} 9.2 & -0.2 \\ -0.2 & 13.2 \end{array} \right) \\
   &= \left( \begin{array}{cc} 13.2 & -1.2 \\ -1.2 & 22 \end{array} \right)
   \end{aligned}
   $$

4. 计算**类间散度矩阵**
   $$
   \begin{aligned}
   S_b &= (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T \\
   &= \left[ \left( \begin{array}{c} 3 \\ 3.8 \end{array} \right) - \left( \begin{array}{c} 8.4 \\ 7.6 \end{array} \right) \right] \left[ \left( \begin{array}{c} 3 \\ 3.8 \end{array} \right) - \left( \begin{array}{c} 8.4 \\ 7.6 \end{array} \right) \right]^T \\
   &= \left( \begin{array}{c} -5.4 \\ -3.8 \end{array} \right) \left( \begin{array}{c} -5.4 \ -3.8 \end{array} \right) \\
   &= \left( \begin{array}{cc} 29.16 & 20.52 \\ 20.52 & 14.44 \end{array} \right)
   \end{aligned}
   $$

5. 计算 $\mathbf{w^*}$
   $$
   \begin{aligned}
   w^* &= S_w^{-1} (\mu_1 - \mu_2) \\
   &= \left( \begin{array}{cc} 3.3 & -0.3 \\ -0.3 & 5.5 \end{array} \right)^{-1} \left[ \left( \begin{array}{c} 3 \\ 3.8 \end{array} \right) - \left( \begin{array}{c} 8.4 \\ 7.6 \end{array} \right) \right] \\
   &= \left( \begin{array}{cc} 0.3045 & 0.0166 \\ 0.0166 & 0.1827 \end{array} \right) \left( \begin{array}{c} -5.4 \\ -3.8 \end{array} \right) \\
   &= \left( \begin{array}{c} 0.9088 \\ 0.4173 \end{array} \right)
   \end{aligned}
   $$





## 3 神经网络Batch Size, Iteration, Epoch计算

- **Batch size(批量大小)**: 即1次迭代使用的样本量
- **Iteration(迭代)**: 1次iteration, 即使用1个batch size样本训练网络参数1次
- **Epoch (期、轮)**: 1个epoch表示遍历了1遍训练集中的所有样本

计算公式：
$$
\text{one epoch} = \text{number of iterations} = N = \text{训练样本的数量} \ / \ \text{batch\_size}
$$

例子：

<img src="https://raw.githubusercontent.com/abecedarian007/picgo_images/master/img/image-20240530120908080.png" style="zoom:67%;" />





## 4 特征图大小计算

- 输入图像大小: $H_{in} \times W_{in} \times n_c$
- 每个滤波器(卷积核)大小: $k \times k \times n_c$
- 滤波器(卷积核)个数: $K$
- 加边填充 padding: $P$
- 卷积核滑动步幅(stride): $S$

### 卷积后输出特征图大小计算

$$
\left( \left\lfloor \frac{H_{in} - k + 2P}{S} \right\rfloor + 1 \right) \times \left( \left\lfloor \frac{W_{in} - k + 2P}{S} \right\rfloor + 1 \right) \times K
$$

计算卷积层输出特征图大小，当除不尽时，一般**向下取整**。

### 池化后输出特征图大小计算

$$
\left( \left\lceil \frac{H_{in} - k + 2P}{S} \right\rceil + 1 \right) \times \left( \left\lceil \frac{W_{in} - k + 2P}{S} \right\rceil + 1 \right) \times K
$$

计算池化层输出特征图大小，当除不尽时，通常**向上取整**。





## 5 CNN 感受野计算

CNN 感受野计算公式：
$$
RF_l = RF_{l-1} + \left( f_l - 1 \right) \times \prod_{i=1}^{l-1} s_i
$$

- 其中 $l$ 为卷积层数； $RF_l$ 为层 $l$ 的感受野，$RF_0 = 1$； $f_l$ 为层 $l$ 的卷积核尺寸； $s_i$ 为层 $i$ 的步幅， $s_0 = 1$。

> 感受野理解：
>
> 感受野表示当前一个特征点能在原图上感受到的范围。
>
> - 前一层的感受野 $RF_{l-1}$：当前层的感受野是基于前一层感受野的。每一层的卷积都会在前一层的基础上<u>扩展感受野</u>。
> - $\left( f_l - 1 \right)$：卷积核尺寸 $f_l$ 表示在当前层一个特征点对应到前一层的感受范围。其中，$\left( f_l - 1 \right)$ 表示除去<u>当前特征点自身所覆盖的感受范围</u>之外，卷积核的**额外感受范围**。
> - $\prod_{i=1}^{l-1} s_i$：前 $l−1$ 层所有步幅的乘积。步幅决定了卷积核每次移动的距离，因此所有步幅的乘积表示<u>每一层卷积在前一层特征图上的**扩展效果**</u>。
> - $\left( f_l - 1 \right) \times \prod_{i=1}^{l-1} s_i$：在当前层，一个特征点的感受野不只是前一层的一个特征点，而是卷积核大小乘以前面所有层步幅的**累积**。这表示<u>当前层一个特征点对应到前一层的更大范围</u>。



## 6 神经网络复杂度计算

### 1）参数量计算

卷积层、全连接层有参数，池化层无参数。

<img src="https://raw.githubusercontent.com/abecedarian007/picgo_images/master/img/20240530203422.png" style="zoom:67%;" />

- 网络权重参数量（仅权重）：
  $$
  5 \times 5 \times 1 \times 6 + 5 \times 5 \times 6 \times 16 + 5 \times 5 \times 16 \times 120 + 120 \times 84 + 84 \times 10
  $$

- 网络全部参数量（权重+偏置）：
  $$
  5 \times 5 \times 1 \times 6 \text{(卷积核)} + 6 \text{(偏置)} + 5 \times 5 \times 6 \times 16 \text{(卷积核)} + 16 \text{(偏置)} + 5 \times 5 \times 16 \times 120 \text{(卷积核)} + 120 \text{(偏置)} \\
  + 120 \times 84 \text{(全连接权重)} + 84 \text{(偏置)} + 84 \times 10 \text{(全连接权重)} + 10 \text{(偏置)}
  $$

### 2）浮点计算量

衡量卷积计算量的指标是 **浮点运算次数**(FLOPs, Floating Point Operations)，**一次乘法和一次加法表示一个浮点运算次数**。

#### CNN浮点计算量

CNN网络：

- 输入图像大小：$H_{in} \times W_{in} \times n_c$
- 输出图像大小：$H_{out} \times W_{out} \times K$
- 每个滤波器（卷积核）大小：$k \times k \times n_c$
- 滤波器（卷积核）个数：$K$
- 加边填充（padding）：$P$
- 卷积滑动步幅（stride）：$S$

CNN网络计算量：

- 卷积核每滑动一次的乘法浮点计算量：$k \times k \times n_c$
- 卷积核每滑动一次的加法浮点计算量：$k \times k \times n_c - 1$
- 输出单个特征图的卷积乘法浮点计算量：$(k \times k \times n_c) \times H_{out} \times W_{out}$
- 输出单个特征图的卷积加法浮点计算量：$(k \times k \times n_c - 1) \times H_{out} \times W_{out}$
- 输出 $K$ 个特征图的卷积乘法浮点计算量：$(k \times k \times n_c) \times H_{out} \times W_{out} \times K$
- 输出 $K$ 个特征图的卷积加法浮点计算量：$(k \times k \times n_c - 1) \times H_{out} \times W_{out} \times K$
- 输出单个特征图的偏置浮点加法计算量：$H_{out} \times W_{out}$
- 输出 $K$ 个特征图的偏置浮点加法计算量：$H_{out} \times W_{out} \times K$

CNN单个卷积层的**乘法**和**加法**的浮点计算量：
$$
\begin{aligned}
FLOPs &= [(k \times k \times n_c) + (k \times k \times n_c - 1) + 1] \times H_{out} \times W_{out} \times K \\ 
&= 2 \times k \times k \times n_c \times H_{out} \times W_{out} \times K
\end{aligned}
$$
在计算机视觉论文中，常常将一个 **'乘-加'** 组合视为一次浮点运算，英文表述为 **'Multi-Add'**，运算量正好是上面的算法减半，此时的运算量为：
$$
FLOPs =  k \times k \times n_c \times H_{out} \times W_{out} \times K
$$

#### 全连接层的参数量和浮点计算量

$N_{in}$ 表示输入层神经元个数，$N_{out}$ 表示输出层神经元个数。

<img src="https://raw.githubusercontent.com/abecedarian007/picgo_images/master/img/20240530211641.png" style="zoom:50%;" />

单层全连接层的网络模型**参数量**（权重+偏置）： 
$$
parameters = (N_{in} + 1) \times N_{out}
$$
单个全连接层的乘法和加法**浮点计算量**（权重+偏置）：
$$
FLOPs = [N_{in} + (N_{in} - 1) + 1] \times N_{out} = 2 \times N_{in} \times N_{out}
$$

> 上述公式中第一个 $N_{in}$ 表示乘法运算量，$N_{in} - 1$ 表示加法运算量，+1 表示 $N_{out}$ 个偏置项计算量，$\times N_{out}$ 表示计算 $N_{out}$ 个神经元的值。

如果将一个 **‘乘-加’** 组合视为一次浮点运算，则此时单个全连接层的**浮点运算量**为：
$$
FLOPs = N_{in} \times N_{out}
$$



## 7 SVM最大间隔分离超平面计算

凸二次规划问题：
$$
\min_{\mathbf{w}, b} \frac{1}{ \|\mathbf{w}\|^2_2 } \quad \text{s.t.} \quad y_i (\mathbf{w}^T \mathbf{x}_i + b) \ge 1, \quad i = 1, 2, \ldots, m
$$
例题：

<img src="https://raw.githubusercontent.com/abecedarian007/picgo_images/master/img/20240531151315.png" style="zoom: 50%;" />
