# 计算题



## 1 相似性计算

### 1) 简单系数匹配

处理<u>对称</u>的二值离散型属性
$$
\text{SMC} = \frac{\text{值匹配的属性个数}}{\text{属性个数}} 
= \frac{f_{11} + f_{00}}{f_{01} + f_{10} + f_{11} + f_{00}}
$$

### 2) Jaccard 系数

处理包含<u>非对称</u>的二值离散型属性
$$
J = \frac{\text{匹配个数}}{\text{00匹配中不涉及的属性个数}} 
= \frac{f_{11}}{f_{01} + f_{10} + f_{11}}
$$

例题：

<img src="https://raw.githubusercontent.com/Kaiming-Y/picgo_images/master/img/20240527221026.png" style="zoom: 33%;" />

### 3) 余弦相似度

$$
\cos(\mathbf{x}, \mathbf{y}) = \frac{\langle \mathbf{x}, \mathbf{y} \rangle}{\|\mathbf{x}\| \|\mathbf{y}\|}
$$

其中，
$$
\langle \mathbf{x}, \mathbf{y} \rangle = \sum_{k=1}^{n} x_k y_k = \mathbf{x}^T \mathbf{y}
$$

$$
\|\mathbf{x}\| = \sqrt{ \sum_{k=1}^{n} x_k^2 } = \sqrt{ \langle \mathbf{x}, \mathbf{x} \rangle } = \sqrt{ \mathbf{x}' \mathbf{x} } \\
\|\mathbf{y}\| = \sqrt{ \sum_{k=1}^{n} y_k^2 } = \sqrt{ \langle \mathbf{y}, \mathbf{y} \rangle } = \sqrt{ \mathbf{y}' \mathbf{y} }
$$

例题：

<img src="https://raw.githubusercontent.com/Kaiming-Y/picgo_images/master/img/20240527221443.png" style="zoom: 33%;" />

### 4) 皮尔森相关系数

度量两个变量之间的**线性相关性**：<u>协方差与标准差之比</u>
$$
\text{corr}(\mathbf{x}, \mathbf{y}) = \frac{\text{covariance}(\mathbf{x}, \mathbf{y})}{\text{standard\_deviation}(\mathbf{x}) \times \text{standard\_deviation}(\mathbf{y})} = \frac{s_{xy}}{s_x s_y}
$$

$$
\text{covariance}(\mathbf{x}, \mathbf{y}) = s_{xy} = \frac{1}{n-1} \sum_{k=1}^{n} (x_k - \overline{x})(y_k - \overline{y})
$$

$$
\text{standard\_deviation}(\mathbf{x}) = s_x = \sqrt{ \frac{1}{n-1} \sum_{k=1}^{n} (x_k - \overline{x})^2 }
$$

$$
\text{standard\_deviation}(\mathbf{y}) = s_y = \sqrt{ \frac{1}{n-1} \sum_{k=1}^{n} (y_k - \overline{y})^2 }
$$

$$
\overline{x} = \frac{1}{n} \sum_{k=1}^{n} x_k \quad \text{是} \quad \mathbf{x} \quad \text{的均值}
$$

$$
\overline{y} = \frac{1}{n} \sum_{k=1}^{n} y_k \quad \text{是} \quad \mathbf{y} \quad \text{的均值}
$$

皮尔森系数的局限性：对于**非线性的相关性**难以建模

例题：

<img src="https://raw.githubusercontent.com/Kaiming-Y/picgo_images/master/img/20240527223255.png" style="zoom: 33%;" />





## 2 LDA 计算

#### 题目

由两类二维数据计算线性判别分析(LDA)投影向量

第一类采样数据 $ \omega_1 $ ： $\mathbf{X_1} = (\mathbf{x_1}, \mathbf{x_2}) = \{(4,2), (2,4), (2,3), (3,6), (4,4)\} \quad$

第二类采样数据 $ \omega_2 $ ： $\mathbf{X_2} = (\mathbf{x_1}, \mathbf{x_2}) = \{(9,10), (6,8), (9,5), (8,7), (10,8)\} \quad$

<img src="https://raw.githubusercontent.com/Kaiming-Y/picgo_images/master/img/20240529203718.png" style="zoom:50%;" />

#### 解

1. 计算**均值**
   $$
   \mu_1 = \frac{1}{N_1} \sum_{\mathbf{x} \in \omega_1} \mathbf{x} = \frac{1}{5} \left[ \left( \begin{array}{c} 4 \\ 2 \end{array} \right) + \left( \begin{array}{c} 2 \\ 4 \end{array} \right) + \left( \begin{array}{c} 2 \\ 3 \end{array} \right) + \left( \begin{array}{c} 3 \\ 6 \end{array} \right) + \left( \begin{array}{c} 4 \\ 4 \end{array} \right) \right] = \left( \begin{array}{c} 3 \\ 3.8 \end{array} \right)
   $$

   $$
   \mu_2 = \frac{1}{N_2} \sum_{\mathbf{x} \in \omega_2} \mathbf{x} = \frac{1}{5} \left[ \left( \begin{array}{c} 9 \\ 10 \end{array} \right) + \left( \begin{array}{c} 6 \\ 8 \end{array} \right) + \left( \begin{array}{c} 9 \\ 5 \end{array} \right) + \left( \begin{array}{c} 8 \\ 7 \end{array} \right) + \left( \begin{array}{c} 10 \\ 8 \end{array} \right) \right] = \left( \begin{array}{c} 8.4 \\ 7.6 \end{array} \right)
   $$

2. 计算**类内散度矩阵**
   $$
   \begin{aligned}
   S_1 &= \sum_{\mathbf{x} \in \omega_1} (\mathbf{x} - \mu_1)(\mathbf{x} - \mu_1)^T \\
   &= \left[ \left( \begin{array}{c} 4 \\ 2 \end{array} \right) - \left( \begin{array}{c} 3 \\ 3.8 \end{array} \right) \right]^2 + \left[ \left( \begin{array}{c} 2 \\ 4 \end{array} \right) - \left( \begin{array}{c} 3 \\ 3.8 \end{array} \right) \right]^2 + \left[ \left( \begin{array}{c} 2 \\ 3 \end{array} \right) - \left( \begin{array}{c} 3 \\ 3.8 \end{array} \right) \right]^2 + \left[ \left( \begin{array}{c} 3 \\ 6 \end{array} \right) - \left( \begin{array}{c} 3 \\ 3.8 \end{array} \right) \right]^2 + \left[ \left( \begin{array}{c} 4 \\ 4 \end{array} \right) - \left( \begin{array}{c} 3 \\ 3.8 \end{array} \right) \right]^2 \\
   &= \left( \begin{array}{cc} 4 & -1 \\ -1 & 8.8 \end{array} \right)
   \end{aligned}
   $$

   $$
   \begin{aligned}
   S_2 &= \sum_{\mathbf{x} \in \omega_2} (\mathbf{x} - \mu_2)(\mathbf{x} - \mu_2)^T \\
   &= \left[ \left( \begin{array}{c} 9 \\ 10 \end{array} \right) - \left( \begin{array}{c} 8.4 \\ 7.6 \end{array} \right) \right]^2 + \left[ \left( \begin{array}{c} 6 \\ 8 \end{array} \right) - \left( \begin{array}{c} 8.4 \\ 7.6 \end{array} \right) \right]^2 + \left[ \left( \begin{array}{c} 9 \\ 5 \end{array} \right) - \left( \begin{array}{c} 8.4 \\ 7.6 \end{array} \right) \right]^2 + \left[ \left( \begin{array}{c} 8 \\ 7 \end{array} \right) - \left( \begin{array}{c} 8.4 \\ 7.6 \end{array} \right) \right]^2 + \left[ \left( \begin{array}{c} 10 \\ 8 \end{array} \right) - \left( \begin{array}{c} 8.4 \\ 7.6 \end{array} \right) \right]^2 \\
   &= \left( \begin{array}{cc} 9.2 & -0.2 \\ -0.2 & 13.2 \end{array} \right)
   \end{aligned}
   $$

3. 计算**总类内散度矩阵**
   $$
   \begin{aligned}
   S_w &= S_1 + S_2 \\
   &= \left( \begin{array}{cc} 4 & -1 \\ -1 & 8.8 \end{array} \right) + \left( \begin{array}{cc} 9.2 & -0.2 \\ -0.2 & 13.2 \end{array} \right) \\
   &= \left( \begin{array}{cc} 13.2 & -1.2 \\ -1.2 & 22 \end{array} \right)
   \end{aligned}
   $$

4. 计算**类间散度矩阵**
   $$
   \begin{aligned}
   S_b &= (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T \\
   &= \left[ \left( \begin{array}{c} 3 \\ 3.8 \end{array} \right) - \left( \begin{array}{c} 8.4 \\ 7.6 \end{array} \right) \right] \left[ \left( \begin{array}{c} 3 \\ 3.8 \end{array} \right) - \left( \begin{array}{c} 8.4 \\ 7.6 \end{array} \right) \right]^T \\
   &= \left( \begin{array}{c} -5.4 \\ -3.8 \end{array} \right) \left( \begin{array}{c} -5.4 \ -3.8 \end{array} \right) \\
   &= \left( \begin{array}{cc} 29.16 & 20.52 \\ 20.52 & 14.44 \end{array} \right)
   \end{aligned}
   $$

5. 计算 $\mathbf{w^*}$
   $$
   \begin{aligned}
   w^* &= S_w^{-1} (\mu_1 - \mu_2) \\
   &= \left( \begin{array}{cc} 3.3 & -0.3 \\ -0.3 & 5.5 \end{array} \right)^{-1} \left[ \left( \begin{array}{c} 3 \\ 3.8 \end{array} \right) - \left( \begin{array}{c} 8.4 \\ 7.6 \end{array} \right) \right] \\
   &= \left( \begin{array}{cc} 0.3045 & 0.0166 \\ 0.0166 & 0.1827 \end{array} \right) \left( \begin{array}{c} -5.4 \\ -3.8 \end{array} \right) \\
   &= \left( \begin{array}{c} 0.9088 \\ 0.4173 \end{array} \right)
   \end{aligned}
   $$





## 3 神经网络Batch Size, Iteration, Epoch计算

- **Batch size(批量大小)**: 即1次迭代使用的样本量
- **Iteration(迭代)**: 1次iteration, 即使用1个batch size样本训练网络参数1次
- **Epoch (期、轮)**: 1个epoch表示遍历了1遍训练集中的所有样本

计算公式：
$$
\text{one epoch} = \text{number of iterations} = N = \text{训练样本的数量} \ / \ \text{batch\_size}
$$

例子：

<img src="https://raw.githubusercontent.com/Kaiming-Y/picgo_images/master/img/image-20240530120908080.png" style="zoom:67%;" />





## 4 特征图大小计算

- 输入图像大小: $H_{in} \times W_{in} \times n_c$
- 每个滤波器(卷积核)大小: $k \times k \times n_c$
- 滤波器(卷积核)个数: $K$
- 加边填充 padding: $P$
- 卷积核滑动步幅(stride): $S$

### 卷积后输出特征图大小计算

$$
\left( \left\lfloor \frac{H_{in} - k + 2P}{S} \right\rfloor + 1 \right) \times \left( \left\lfloor \frac{W_{in} - k + 2P}{S} \right\rfloor + 1 \right) \times K
$$

计算卷积层输出特征图大小，当除不尽时，一般**向下取整**。

### 池化后输出特征图大小计算

$$
\left( \left\lceil \frac{H_{in} - k + 2P}{S} \right\rceil + 1 \right) \times \left( \left\lceil \frac{W_{in} - k + 2P}{S} \right\rceil + 1 \right) \times K
$$

计算池化层输出特征图大小，当除不尽时，通常**向上取整**。





## 5 CNN 感受野计算

CNN 感受野计算公式：
$$
RF_l = RF_{l-1} + \left( f_l - 1 \right) \times \prod_{i=1}^{l-1} s_i
$$

- 其中 $l$ 为卷积层数； $RF_l$ 为层 $l$ 的感受野，$RF_0 = 1$； $f_l$ 为层 $l$ 的卷积核尺寸； $s_i$ 为层 $i$ 的步幅， $s_0 = 1$。

> 感受野理解：
>
> 感受野表示当前一个特征点能在原图上感受到的范围。
>
> - 前一层的感受野 $RF_{l-1}$：当前层的感受野是基于前一层感受野的。每一层的卷积都会在前一层的基础上<u>扩展感受野</u>。
> - $\left( f_l - 1 \right)$：卷积核尺寸 $f_l$ 表示在当前层一个特征点对应到前一层的感受范围。其中，$\left( f_l - 1 \right)$ 表示除去<u>当前特征点自身所覆盖的感受范围</u>之外，卷积核的**额外感受范围**。
> - $\prod_{i=1}^{l-1} s_i$：前 $l−1$ 层所有步幅的乘积。步幅决定了卷积核每次移动的距离，因此所有步幅的乘积表示<u>每一层卷积在前一层特征图上的**扩展效果**</u>。
> - $\left( f_l - 1 \right) \times \prod_{i=1}^{l-1} s_i$：在当前层，一个特征点的感受野不只是前一层的一个特征点，而是卷积核大小乘以前面所有层步幅的**累积**。这表示<u>当前层一个特征点对应到前一层的更大范围</u>。



## 6 神经网络复杂度计算

### 1）参数量计算

卷积层、全连接层有参数，池化层无参数。

<img src="https://raw.githubusercontent.com/Kaiming-Y/picgo_images/master/img/20240530203422.png" style="zoom:67%;" />

- 网络权重参数量（仅权重）：
  $$
  5 \times 5 \times 1 \times 6 + 5 \times 5 \times 6 \times 16 + 5 \times 5 \times 16 \times 120 + 120 \times 84 + 84 \times 10
  $$

- 网络全部参数量（权重+偏置）：
  $$
  5 \times 5 \times 1 \times 6 \text{(卷积核)} + 6 \text{(偏置)} + 5 \times 5 \times 6 \times 16 \text{(卷积核)} + 16 \text{(偏置)} + 5 \times 5 \times 16 \times 120 \text{(卷积核)} + 120 \text{(偏置)} \\
  + 120 \times 84 \text{(全连接权重)} + 84 \text{(偏置)} + 84 \times 10 \text{(全连接权重)} + 10 \text{(偏置)}
  $$

### 2）浮点计算量

衡量卷积计算量的指标是 **浮点运算次数**(FLOPs, Floating Point Operations)，**一次乘法和一次加法表示一个浮点运算次数**。

#### CNN浮点计算量

CNN网络：

- 输入图像大小：$H_{in} \times W_{in} \times n_c$
- 输出图像大小：$H_{out} \times W_{out} \times K$
- 每个滤波器（卷积核）大小：$k \times k \times n_c$
- 滤波器（卷积核）个数：$K$
- 加边填充（padding）：$P$
- 卷积滑动步幅（stride）：$S$

CNN网络计算量：

- 卷积核每滑动一次的乘法浮点计算量：$k \times k \times n_c$
- 卷积核每滑动一次的加法浮点计算量：$k \times k \times n_c - 1$
- 输出单个特征图的卷积乘法浮点计算量：$(k \times k \times n_c) \times H_{out} \times W_{out}$
- 输出单个特征图的卷积加法浮点计算量：$(k \times k \times n_c - 1) \times H_{out} \times W_{out}$
- 输出 $K$ 个特征图的卷积乘法浮点计算量：$(k \times k \times n_c) \times H_{out} \times W_{out} \times K$
- 输出 $K$ 个特征图的卷积加法浮点计算量：$(k \times k \times n_c - 1) \times H_{out} \times W_{out} \times K$
- 输出单个特征图的偏置浮点加法计算量：$H_{out} \times W_{out}$
- 输出 $K$ 个特征图的偏置浮点加法计算量：$H_{out} \times W_{out} \times K$

CNN单个卷积层的**乘法**和**加法**的浮点计算量：
$$
\begin{aligned}
FLOPs &= [(k \times k \times n_c) + (k \times k \times n_c - 1) + 1] \times H_{out} \times W_{out} \times K \\ 
&= 2 \times k \times k \times n_c \times H_{out} \times W_{out} \times K
\end{aligned}
$$
在计算机视觉论文中，常常将一个 **'乘-加'** 组合视为一次浮点运算，英文表述为 **'Multi-Add'**，运算量正好是上面的算法减半，此时的运算量为：
$$
FLOPs =  k \times k \times n_c \times H_{out} \times W_{out} \times K
$$

#### 全连接层的参数量和浮点计算量

$N_{in}$ 表示输入层神经元个数，$N_{out}$ 表示输出层神经元个数。

<img src="https://raw.githubusercontent.com/Kaiming-Y/picgo_images/master/img/20240530211641.png" style="zoom:50%;" />

单层全连接层的网络模型**参数量**（权重+偏置）： 
$$
parameters = (N_{in} + 1) \times N_{out}
$$
单个全连接层的乘法和加法**浮点计算量**（权重+偏置）：
$$
FLOPs = [N_{in} + (N_{in} - 1) + 1] \times N_{out} = 2 \times N_{in} \times N_{out}
$$

> 上述公式中第一个 $N_{in}$ 表示乘法运算量，$N_{in} - 1$ 表示加法运算量，+1 表示 $N_{out}$ 个偏置项计算量，$\times N_{out}$ 表示计算 $N_{out}$ 个神经元的值。

如果将一个 **‘乘-加’** 组合视为一次浮点运算，则此时单个全连接层的**浮点运算量**为：
$$
FLOPs = N_{in} \times N_{out}
$$



## 7 SVM最大间隔分离超平面计算

凸二次规划问题：
$$
\min_{\mathbf{w}, b} \frac{1}{ \|\mathbf{w}\|^2_2 } \quad \text{s.t.} \quad y_i (\mathbf{w}^T \mathbf{x}_i + b) \ge 1, \quad i = 1, 2, \ldots, m
$$
例题：

<img src="https://raw.githubusercontent.com/Kaiming-Y/picgo_images/master/img/20240531151315.png" style="zoom: 50%;" />





## 8 二范数求导

**问题**：

$x \in \mathbb{R}^n$ 是 $n$ 维列向量，$y \in \mathbb{R}^n$，$A = (a_{ij})_{n \times n} \in \mathbb{R}^{n \times n}$ 是 $n \times n$ 矩阵且为对称矩阵，即 $A^T = A$，求 $\dfrac{\partial \|Ax - y\|_2^2}{\partial x}$。

**解**：

我们需要求解的梯度是函数 $\|Ax - y\|_2^2$ 关于向量 $x$ 的偏导数。这里 $x \in \mathbb{R}^n$ 是一个 $n$ 维列向量，$y \in \mathbb{R}^n$， $A$ 是一个 $n \times n$ 对称矩阵，即 $A = A^T$。

定义函数 $f(x) = \|Ax - y\|_2^2$，我们可以展开并简化这个函数：

$$
f(x) = \|Ax - y\|_2^2 = (Ax - y)^T (Ax - y)
$$

我们需要计算 $\dfrac{\partial f}{\partial x}$。首先，展开上面的表达式：

$$
f(x) = (Ax - y)^T (Ax - y) = (Ax)^T (Ax) - (Ax)^T y - y^T (Ax) + y^T y
$$

由于 $y^T y$ 是一个常数项，不影响偏导数的计算，可以忽略。注意到 $y^T (Ax)$ 是一个标量，等于 $(Ax)^T y$，所以我们可以将其合并：

$$
f(x) = x^T A^T A x - 2y^T A x + y^T y
$$

因为 $A$ 是对称矩阵，所以 $A^T = A$，简化为：

$$
f(x) = x^T A A x - 2y^T A x + y^T y = x^T A^2 x - 2y^T A x + y^T y
$$

现在我们对 $x$ 求偏导数：

$$
\frac{\partial f}{\partial x} = \frac{\partial}{\partial x} (x^T A^2 x - 2y^T A x)
$$

使用矩阵微分公式：

$$
\frac{\partial}{\partial x} (x^T A^2 x) = 2A^2 x
$$

$$
\frac{\partial}{\partial x} (-2y^T A x) = -2A^T y = -2Ay
$$

因为 $A$ 是对称矩阵，$A^T = A$。所以我们得到：

$$
\frac{\partial f}{\partial x} = 2Ax - 2Ay = 2A(Ax - y)
$$

综上所述，函数 $\|Ax - y\|_2^2$ 关于 $x$ 的梯度为：

$$
\frac{\partial \|Ax - y\|_2^2}{\partial x} = 2A(Ax - y)
$$




## 9 Aprior频繁模式挖掘

要求规则最小支持度=50%(即支持度计数≥2), 置信度≥70%

**生成频繁项集过程**：

<img src="https://raw.githubusercontent.com/Kaiming-Y/picgo_images/master/img/20240601145359.png" style="zoom:50%;" />

**最终所有频繁项集**：

<img src="https://raw.githubusercontent.com/Kaiming-Y/picgo_images/master/img/20240601145539.png" style="zoom:50%;" />

**找出所有强规则**：

- $\{1, 3\}$ 产生规则：
  - ==$1 \rightarrow 3$ (sup = 2/4 = 50%, conf = 2/2 = 100%)==
  - $3 \rightarrow 1$ (sup = 2/4 = 50%, conf = 2/3 ≈ 66.7%)

- $\{2, 3\}$ 产生规则：
  - $2 \rightarrow 3$ (sup = 2/4 = 50%, conf = 2/3 ≈ 66.7%)
  - $3 \rightarrow 2$ (sup = 2/4 = 50%, conf = 2/3 ≈ 66.7%)

- $\{2, 5\}$ 产生规则：
  - ==$2 \rightarrow 5$ (sup = 3/4 = 75%, conf = 3/3 = 100%)==
  - ==$5 \rightarrow 2$ (sup = 3/4 = 75%, conf = 3/3 = 100%)==

- $\{3, 5\}$ 产生规则：
  - $3 \rightarrow 5$ (sup = 2/4 = 50%, conf = 2/3 ≈ 66.7%)
  - $5 \rightarrow 3$ (sup = 2/4 = 50%, conf = 2/3 ≈ 66.7%)

- $\{2, 3, 5\}$ 产生规则：
  - $2 \rightarrow 3 \cup 5$ (sup = 2/4 = 50%, conf = 2/3 ≈ 66.7%)
  - $3 \rightarrow 2 \cup 5$ (sup = 2/4 = 50%, conf = 2/3 ≈ 66.7%)
  - $5 \rightarrow 2 \cup 3$ (sup = 2/4 = 50%, conf = 2/3 ≈ 66.7%)
  - ==$2 \cup 3 \rightarrow 5$ (sup = 2/4 = 50%, conf = 2/2 = 100%)==
  - $2 \cup 5 \rightarrow 3$ (sup = 2/4 = 50%, conf = 2/3 ≈ 66.7%)
  - ==$3 \cup 5 \rightarrow 2$ (sup = 2/4 = 50%, conf = 2/2 ≈ 100%)==

**强关联规则**：

- $1 \rightarrow 3$ (50%, 100%)
- $2 \rightarrow 5$ (75%, 100%)
- $5 \rightarrow 2$ (75%, 100%)
- $2 \cup 3 \rightarrow 5$ (50%, 100%)
- $3 \cup 5 \rightarrow 2$ (50%, 100%)





## 10 Aprior序列模式挖掘

1. 候选生成

   合并频繁(k-1)-序列，产生候选k-序列。

   <img src="https://raw.githubusercontent.com/Kaiming-Y/picgo_images/master/img/20240601161146.png" style="zoom:60%;" />

   例如：

   - $<\{1\} \{2\} \{3\} \{4\}>$ 是通过合并 $<\{1\} \{2\} \{3\}>$ 和 $<\{2\} \{3\} \{4\}>$ 得到。由于事件 3 和事件 4 属于第二个序列的不同元素，它们在合并后序列中也属于不同的元素。
   - $<\{1\} \{5\} \{3,4\}>$ 是通过合并 $<\{1\} \{5\} \{3\}>$ 和 $<\{5\} \{3,4\}>$ 得到。由于事件 3 和事件 4 属于第二个序列的相同元素，4 被合并到第一个序列的最后一个元素中。

2. 候选剪枝

   - 一个候选 $k$-序列被剪枝，如果它的 $(k-1)$-序列最少有一个是非频繁的。
   - 例如，假设 $<\{1\} \{2\} \{3\} \{4\}>$ 是一个候选 4-序列。我们需要检查 $<\{1\} \{2\} \{4\}>$ 和 $<\{1\} \{3\} \{4\}>$ 是否是频繁 3-序列。由于它们都不是频繁的，因此可以删除候选 $<\{1\} \{2\} \{3\} \{4\}>$。

例子1：

<img src="https://raw.githubusercontent.com/Kaiming-Y/picgo_images/master/img/20240601161250.png" style="zoom: 40%;" />

例子2：

考虑以下频繁3-序列：$<\{1,2,3\}>, <\{1,2\}\{3\}>, <\{1\}\{2,3\}>, <\{1,2\}\{4\}>, <\{1,3\}\{4\}>, <\{2,3\}\{3\}>, <\{2,3\}\{4\}>, <\{2\}\{3\}\{3\}>, <\{2\}\{3\}\{4\}>$

1. 列举出候选生成步骤产生的所有候选4-序列
   所有候选4-序列举如下：
   $<\{1,2,3\}\{3\}>, <\{1,2,3\}\{4\}>, <\{1,2\}\{3\}\{3\}>, <\{1,2\}\{3\}\{4\}>, <\{1\},\{2,3\},\{3\},<\{1\}, \{2,3\}, \{4\}>$

2. 列出候选剪枝步骤剪掉的所有候选4-序列（假定没有时间限制）。如果没有时间限制，则所有候选子序列都必须频繁。因此，经过修剪剪掉的候选子序列（非频繁）为：
   $<\{1,2,3\}\{3\}>, <\{1,2\}\{3\}\{3\}>, <\{1,2\},\{3\}\{4\}>, <\{1\},\{2,3\},\{3\},<\{1\}, \{2,3\}, \{4\}>$

剪枝后的候选序列为：$<\{1,2,3\}\{4\}>$





## 11 层次聚类计算

#### 基于距离（相异度）的层次聚类

- **单链**层次聚类步骤（小中取小）

  1. 计算不同簇的两个最近的点之间的距离，再找出距离最小的两个簇，第一个合并；

  2. 按照**“小中取小”**的原则依次合并剩余点，直至合并完所有点。

- **全链**层次聚类步骤（大中取小）
  1. 计算不同簇的两个最远的点之间的距离，再找出距离最小的两个簇，第一个合并；
  2. 按照**“大中取小”**的原则依次合并剩余的点，直至所有点合并完成。

#### 基于相似度矩阵的层次聚类

- **单链**层次聚类步骤（大中取大）
  1. 找出所有点**相似度最大**的两个点，第一个合并；
  2. 按照**“大中取大”**的原则进行合并剩余的点，直至所有点合并完成为止。
- **全链**层次聚类步骤（小中取大）
  1. 找出所有点**相似度最大**的两个点，第一个合并；
  2. 按照**“小中取大”**的原则进行合并剩余的点，直至所有点合并完成为止。

基于距离的层次聚类例子：

1. 单链层次聚类

<img src="https://raw.githubusercontent.com/Kaiming-Y/picgo_images/master/img/20240601210252.png" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/Kaiming-Y/picgo_images/master/img/20240601210307.png" style="zoom:50%;" />

2. 全链层次聚类

<img src="https://raw.githubusercontent.com/Kaiming-Y/picgo_images/master/img/20240601210351.png" style="zoom:50%;" />
